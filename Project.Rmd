---
title: "Project"
output: html_document
date: "2024-11-16"
---

```{r}
install.packages("glmnet")
```

## Read Data

```{r}
library(readr)

# Define the path to the CSV file in the "arxiv" folder
file_path <- file.path("data", "df_train.csv")  # Replace 'your_file.csv' with the actual file name

# Read the CSV file
data_train <- read_csv(file_path)

file_path2 <- file.path("data", "df_test.csv")  # Replace 'your_file.csv' with the actual file name

# Read the CSV file
data_test <- read_csv(file_path2)

# Display the first few rows of the data
print(data_train)
```

## Preprocessing

```{r}
library(dplyr)

# dplyr::filter()  # For the 'filter' function in dplyr
# stats::filter()  # For the 'filter' function in stats
# base::union()    # For the 'union' function in base R

# Convert specific columns to factors with labels "0" and "1"
dataset_train <- data_train %>%
  mutate(across(
    c(has_basement, renovated, nice_view, perfect_condition, has_lavatory, single_floor),
    ~ factor(., labels = c(0, 1))
  ))

dataset_test <- data_test %>%
  mutate(across(
    c(has_basement, renovated, nice_view, perfect_condition, has_lavatory, single_floor),
    ~ factor(., labels = c(0, 1))
  ))
if (!requireNamespace("lubridate", quietly = TRUE)) {
  install.packages("lubridate")
}
library(lubridate)



dataset_test <- dataset_test %>%
  mutate(date = as.numeric(date))  # Converts 'date' column to numeric (days since 1970-01-01)

dataset_train <- dataset_train %>%
  mutate(date = as.numeric(date)) 
```

```{r}

# Replace non-numeric columns in-place
dataset_train <- dataset_train %>%
  mutate(across(everything(), ~ as.numeric(as.character(.))))

# Check for remaining non-numeric columns
non_numeric_cols <- sapply(dataset_train, function(col) !is.numeric(col))
if (any(non_numeric_cols)) {
  print("Non-numeric columns remain:")
  print(names(dataset_train)[non_numeric_cols])
} else {
  print("All columns are now numeric.")
}

print(dataset_train)
```

```{r}
# Install and load ggcorrplot
if (!requireNamespace("ggcorrplot", quietly = TRUE)) {
  install.packages("ggcorrplot")
}
library(ggcorrplot)

# Assuming your dataset is named `dataset_train`
# Calculate the correlation matrix (only numeric columns)
cor_matrix <- cor(dataset_train)

# Plot the correlation matrix with adjusted sizes
ggcorrplot(
  cor_matrix,
  lab = TRUE,               # Show correlation coefficients
  lab_size = 2,             # Larger label size for numbers in the cells
  tl.cex = 8,              # Larger text label size for axis labels
  colors = c("red", "white", "blue"), # Custom color palette
  title = "Correlation Matrix"
)


```

```{r}
ggplot(dataset_train, aes(x = grade, y = living_in_m2, fill = as.factor(grade))) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +  # Change to bar chart
  scale_fill_viridis_d(option = "C") +                           # Use a Viridis color palette
  labs(
    title = "Living Space by grade",
    x = "grade",
    y = "living_space",
    fill = "grade"
  ) +
  theme_minimal() +                                              # Change the theme for a cleaner look
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),           # Center and enlarge the title
    axis.text = element_text(size = 12),                        # Enlarge axis text
    axis.title = element_text(size = 14),                       # Enlarge axis titles
    legend.position = "right"                                   # Move legend to the right
  )

```

```{r}
library(ggplot2)
#install.packages("viridis")

library(viridis)

ggplot(dataset_train, aes(x = bedrooms, y = living_in_m2, fill = as.factor(bedrooms))) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +  # Change to bar chart
  scale_fill_viridis_d(option = "C") +                           # Use a Viridis color palette
  labs(
    title = "Living Space by Number of Bedrooms",
    x = "Number of Bedrooms",
    y = "Living Space (mÂ²)",
    fill = "Bedrooms"
  ) +
  theme_minimal() +                                              # Change the theme for a cleaner look
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),           # Center and enlarge the title
    axis.text = element_text(size = 12),                        # Enlarge axis text
    axis.title = element_text(size = 14),                       # Enlarge axis titles
    legend.position = "right"                                   # Move legend to the right
  )

```

```{r}

ggplot(dataset_train, aes(x = grade, y = bedrooms, fill = as.factor(grade))) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +  # Change to bar chart
  scale_fill_viridis_d(option = "C") +                           # Use a Viridis color palette
  labs(
    title = "bedrooms by grade",
    x = "grade",
    y = "bedrooms",
    fill = "grade"
  ) +
  theme_minimal() +                                              # Change the theme for a cleaner look
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),           # Center and enlarge the title
    axis.text = element_text(size = 12),                        # Enlarge axis text
    axis.title = element_text(size = 14),                       # Enlarge axis titles
    legend.position = "right"                                   # Move legend to the right
  )
```

```{r}

ggplot(dataset_train, aes(x = grade, y = price, fill = as.factor(grade))) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +  # Change to bar chart
  scale_fill_viridis_d(option = "C") +                           # Use a Viridis color palette
  labs(
    title = "price by grade",
    x = "grade",
    y = "price",
    fill = "grade"
  ) +
  theme_minimal() +                                              # Change the theme for a cleaner look
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),           # Center and enlarge the title
    axis.text = element_text(size = 12),                        # Enlarge axis text
    axis.title = element_text(size = 14),                       # Enlarge axis titles
    legend.position = "right"                                   # Move legend to the right
  )
```

```{r}

ggplot(dataset_train, aes(x = bedrooms, y = price, fill = as.factor(bedrooms))) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +  # Change to bar chart
  scale_fill_viridis_d(option = "C") +                           # Use a Viridis color palette
  labs(
    title = "price by bedrooms",
    x = "bedroooms",
    y = "price",
    fill = "grade"
  ) +
  theme_minimal() +                                              # Change the theme for a cleaner look
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),           # Center and enlarge the title
    axis.text = element_text(size = 12),                        # Enlarge axis text
    axis.title = element_text(size = 14),                       # Enlarge axis titles
    legend.position = "right"                                   # Move legend to the right
  )

```

```{r}

ggplot(dataset_train, aes(x = real_bathrooms, y = price, fill = as.factor(real_bathrooms))) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +  # Change to bar chart
  scale_fill_viridis_d(option = "C") +                           # Use a Viridis color palette
  labs(
    title = "price by bathrooms",
    x = "bathroom",
    y = "price",
    fill = "grade"
  ) +
  theme_minimal() +                                              # Change the theme for a cleaner look
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),           # Center and enlarge the title
    axis.text = element_text(size = 12),                        # Enlarge axis text
    axis.title = element_text(size = 14),                       # Enlarge axis titles
    legend.position = "right"                                   # Move legend to the right
  )
```

```{r}

ggplot(dataset_train, aes(x = nice_view, y = price, fill = as.factor(nice_view))) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +  # Change to bar chart
  scale_fill_viridis_d(option = "C") +                           # Use a Viridis color palette
  labs(
    title = "price by nice view",
    x = "nice view",
    y = "price",
    fill = "nice view"
  ) +
  theme_minimal() +                                              # Change the theme for a cleaner look
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),           # Center and enlarge the title
    axis.text = element_text(size = 12),                        # Enlarge axis text
    axis.title = element_text(size = 14),                       # Enlarge axis titles
    legend.position = "right"                                   # Move legend to the right
  )
```

```{r}

ggplot(dataset_train, aes(x = quartile_zone, y = price, fill = as.factor(quartile_zone))) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +  # Change to bar chart
  scale_fill_viridis_d(option = "C") +                           # Use a Viridis color palette
  labs(
    title = "price by zone",
    x = "zone",
    y = "price",
    fill = "zone"
  ) +
  theme_minimal() +                                              # Change the theme for a cleaner look
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),           # Center and enlarge the title
    axis.text = element_text(size = 12),                        # Enlarge axis text
    axis.title = element_text(size = 14),                       # Enlarge axis titles
    legend.position = "right"                                   # Move legend to the right
  )
```

```{r}
library(MASS)
library(glmnet)

model <- lm(price ~ ., data = dataset_train)
step <- stepAIC(model, direction = "both")
# summary(step)
print("-------------------------------------------------")

x <- model.matrix(price ~ ., dataset_train)[,-1]
y <- dataset_train$price
lasso_model <- cv.glmnet(x, y, alpha = 1) # Lasso
ridge_model <- cv.glmnet(x, y, alpha = 0) # Ridge

lasso_model
print("-------------------------------------------------")
ridge_model
print("-------------------------------------------------")


```

```{r}
# Load required library
library(glmnet)

# Assuming `dataset` is your data frame
# Extract the predictor matrix (X) and response variable (y)
X <- model.matrix(price ~ ., dataset_train)[, -1]  # Exclude the intercept
y <- dataset_train$price

# Ridge Regression (alpha = 0)
set.seed(123)  # For reproducibility
ridge_model <- cv.glmnet(X, y, alpha = 0)  # Cross-validation for Ridge
ridge_best_lambda <- ridge_model$lambda.min  # Best lambda for Ridge

# Extract coefficients for the best lambda in Ridge
ridge_coefficients <- as.matrix(coef(ridge_model, s = ridge_best_lambda))  # Convert to matrix
ridge_scores <- abs(ridge_coefficients[, 1])  # Absolute values of coefficients
ridge_ranked_features <- data.frame(Feature = rownames(ridge_coefficients), Score = ridge_scores)

# Lasso Regression (alpha = 1)
set.seed(123)  # For reproducibility
lasso_model <- cv.glmnet(X, y, alpha = 1)  # Cross-validation for Lasso
lasso_best_lambda <- lasso_model$lambda.min  # Best lambda for Lasso

# Extract coefficients for the best lambda in Lasso
lasso_coefficients <- as.matrix(coef(lasso_model, s = lasso_best_lambda))  # Convert to matrix
lasso_scores <- abs(lasso_coefficients[, 1])  # Absolute values of coefficients
lasso_ranked_features <- data.frame(Feature = rownames(lasso_coefficients), Score = lasso_scores)

# Remove intercept from rankings
ridge_ranked_features <- ridge_ranked_features[ridge_ranked_features$Feature != "(Intercept)", ]
lasso_ranked_features <- lasso_ranked_features[lasso_ranked_features$Feature != "(Intercept)", ]

# Sort features by score in descending order
ridge_ranked_features <- ridge_ranked_features[order(-ridge_ranked_features$Score), ]
lasso_ranked_features <- lasso_ranked_features[order(-lasso_ranked_features$Score), ]

# Select top 50% of features
ridge_top_features <- ridge_ranked_features[1:floor(nrow(ridge_ranked_features) / 2), ]
lasso_top_features <- lasso_ranked_features[1:floor(nrow(lasso_ranked_features) / 2), ]

# Print Results
print("Top 50% Features by Ridge Regression:\n")
print(ridge_top_features)

print("\nTop 50% Features by Lasso Regression:\n")
print(lasso_top_features)
```

```{r}
# Greedy algorithm for feature selection based on adjusted R-squared
greedy_feature_selection <- function(data, target_var) {
  # Initialize variables
  features <- setdiff(names(data), target_var)  # All features except target
  selected_features <- c()  # Empty set of selected features
  adj_r_squared_list <- c()  # Store adjusted R-squared at each step
  feature_order <- c()  # Store the order in which features are added
  
  # Iterate until all features are considered
  while (length(features) > 0) {
    best_adj_r_squared <- -Inf
    best_feature <- NULL
    
    # Try adding each feature not in selected_features
    for (feature in features) {
      current_features <- c(selected_features, feature)
      formula <- as.formula(paste(target_var, "~", paste(current_features, collapse = " + ")))
      model <- lm(formula, data = data)
      adj_r_squared <- summary(model)$adj.r.squared
      
      # Update best feature if adjusted R-squared improves
      if (adj_r_squared > best_adj_r_squared) {
        best_adj_r_squared <- adj_r_squared
        best_feature <- feature
      }
    }
    
    # Update selected features and remaining features
    selected_features <- c(selected_features, best_feature)
    features <- setdiff(features, best_feature)
    
    # Store results
    adj_r_squared_list <- c(adj_r_squared_list, best_adj_r_squared)
    feature_order <- c(feature_order, best_feature)
  }
  
  # Combine results into a data frame
  result <- data.frame(
    Step = 1:length(feature_order),
    Feature_Added = feature_order,
    Adjusted_R_Squared = adj_r_squared_list
  )
  
  return(result)
}

# Apply the greedy feature selection function
# Replace 'dataset' with your actual dataset and 'price' with your target variable
result <- greedy_feature_selection(data = dataset_train, target_var = "price")

# Display the results
print(result)
```
```{r}
# Greedy algorithm for feature selection considering original and squared versions
greedy_feature_selection_all <- function(data, target_var) {
  # Initialize variables
  features <- setdiff(names(data), target_var)  # All features except the target
  selected_features <- c()  # Empty set of selected features
  adj_r_squared_list <- c()  # Store adjusted R-squared at each step
  feature_order <- c()  # Store the order in which features are added
  
  # Add squared versions of features to the dataset
  for (feature in features) {
    data[[paste0(feature, "_squared")]] <- data[[feature]]^2
  }
  all_features <- c(features, paste0(features, "_squared"))  # Original + squared versions
  
  # Greedy selection process
  while (length(all_features) > 0) {
    best_adj_r_squared <- -Inf
    best_feature <- NULL
    
    # Try adding each feature not yet in the selected set
    for (feature in all_features) {
      current_features <- c(selected_features, feature)
      formula <- as.formula(paste(target_var, "~", paste(current_features, collapse = " + ")))
      model <- lm(formula, data = data)
      adj_r_squared <- summary(model)$adj.r.squared
      
      # Update the best feature if adjusted R-squared improves
      if (adj_r_squared > best_adj_r_squared) {
        best_adj_r_squared <- adj_r_squared
        best_feature <- feature
      }
    }
    
    # Update selected features and remaining features
    selected_features <- c(selected_features, best_feature)
    all_features <- setdiff(all_features, best_feature)  # Remove selected feature
    
    # Store results
    adj_r_squared_list <- c(adj_r_squared_list, best_adj_r_squared)
    feature_order <- c(feature_order, best_feature)
  }
  
  # Combine results into a data frame
  result <- data.frame(
    Step = 1:length(feature_order),
    Feature_Added = feature_order,
    Adjusted_R_Squared = adj_r_squared_list
  )
  
  return(result)
}

# Apply the greedy feature selection function
# Replace 'dataset' with your actual dataset and 'price' with your target variable
result <- greedy_feature_selection_all(data = dataset_train, target_var = "price")

# Display the results
print(result)

```
```{r}
# Greedy algorithm for interaction term selection
greedy_feature_selection_interactions <- function(data, target_var, current_features) {
  # Initialize variables
  features <- current_features  # Features already selected
  interaction_terms <- combn(features, 2, simplify = FALSE)  # All pairs of features
  adj_r_squared_list <- c()  # Store adjusted R-squared at each step
  interaction_order <- c()  # Store the order in which interaction terms are added
  
  # Greedy selection process for interaction terms
  while (length(interaction_terms) > 0) {
    best_adj_r_squared <- -Inf
    best_interaction <- NULL
    
    # Try adding each interaction term
    for (interaction in interaction_terms) {
      # Create interaction term
      interaction_name <- paste0(interaction[1], "_x_", interaction[2])
      data[[interaction_name]] <- data[[interaction[1]]] * data[[interaction[2]]]
      
      # Fit the model with the new interaction term
      formula <- as.formula(paste(
        target_var, "~", paste(features, collapse = " + "), "+", interaction_name
      ))
      model <- lm(formula, data = data)
      adj_r_squared <- summary(model)$adj.r.squared
      
      # Update the best interaction if adjusted R-squared improves
      if (adj_r_squared > best_adj_r_squared) {
        best_adj_r_squared <- adj_r_squared
        best_interaction <- interaction
      }
    }
    
    # Update selected interactions and remove from the pool
    if (!is.null(best_interaction)) {
      interaction_name <- paste0(best_interaction[1], "_x_", best_interaction[2])
      features <- c(features, interaction_name)
      interaction_order <- c(interaction_order, interaction_name)
      adj_r_squared_list <- c(adj_r_squared_list, best_adj_r_squared)
      
      # Remove the selected interaction from the pool
      interaction_terms <- interaction_terms[!sapply(interaction_terms, function(x) all(x %in% best_interaction))]
    } else {
      break  # Stop if no improvement
    }
  }
  
  # Combine results into a data frame
  result <- data.frame(
    Step = 1:length(interaction_order),
    Interaction_Added = interaction_order,
    Adjusted_R_Squared = adj_r_squared_list
  )
  
  return(result)
}

# Example Usage
# Assuming 'dataset' is your data frame and 'price' is the target variable
# Assuming `current_features` is a list of features already selected in the current model
current_features <- c("nice_view", "quartile_zone", "perfect_condition", "renovated", "grade", "has_basement")
result <- greedy_feature_selection_interactions(data = dataset_train, target_var = "price", current_features = current_features)

# Display the results
print(result)
```
```{r}
# Define the selected features
selected_original_features <- c("nice_view", "quartile_zone", "perfect_condition", "renovated", "grade", "has_basement")

# Define the selected squared features
selected_squared_features <- c("living_in_m2_squared", "perfect_condition_squared")

# Define the selected interaction terms
selected_interaction_terms <- c("grade_x_nice_view", "grade_x_quartile_zone", "renovated_x_grade", "perfect_condition_x_grade")

# Add interaction terms to the dataset
for (interaction in selected_interaction_terms) {
  features <- unlist(strsplit(interaction, "_x_"))
  dataset_train[[interaction]] <- dataset_train[[features[1]]] * dataset_train[[features[2]]]
}

# Add squared terms to the dataset
for (squared_feature in selected_squared_features) {
  original_feature <- gsub("_squared", "", squared_feature)
  dataset_train[[squared_feature]] <- dataset_train[[original_feature]]^2
}

# Combine all selected features
all_selected_features <- c(selected_original_features, selected_squared_features, selected_interaction_terms)

# Create the formula for the final regression model
final_formula <- as.formula(paste("price ~", paste(all_selected_features, collapse = " + ")))

# Fit the Multiple Linear Regression model
final_model <- lm(final_formula, data = dataset_train)

# Display the model summary
summary(final_model)
```
## MLR Model
### Lasso, Ridge, and All features MLR models

```{r}
mlr_model_all <- lm(price ~ bedrooms + grade + has_basement + living_in_m2 + renovated + nice_view +
                  perfect_condition + real_bathrooms + has_lavatory + single_floor +
                  month + quartile_zone, data = dataset_train)

mlr_model_ridge <- lm(price ~ nice_view + quartile_zone + perfect_condition + renovated + grade + has_basement, data = dataset_train)

mlr_model_lasso <- lm(price ~ nice_view + quartile_zone + perfect_condition + renovated + grade + has_lavatory, data = dataset_train)

summary(mlr_model_all)
print("-------------------------------------------------------------")
summary(mlr_model_ridge)
print("-------------------------------------------------------------")
summary(mlr_model_lasso)

```

```{r}

```
### Model Summary


### Interpretations:
- **Significant Predictors**: All predictors have p-values < 0.05, indicating statistical significance.
- **Strongest Predictors**:
  - `living_in_m2`: A critical driver of house prices.
  - `grade`: Higher grades significantly increase house prices.
  - `quartile_zone`: Highlights the importance of location.


- **Residual Standard Error**: \( 0.01665 \), indicating a good fit.
- **Multiple R-squared**: \( 0.7628 \), meaning approximately 76% of the variance in log(price) is explained by the model.
- **Adjusted R-squared**: \( 0.7625 \), accounting for the number of predictors.
- **F-statistic**: \( 3121 \), p-value < \( 2.2e^{-16} \), confirming overall model significance.
```{r}

# Residuals vs Fitted Plot
plot(mlr_model, which = 1, main = "Residuals vs Fitted")

# Normal Q-Q Plot
plot(mlr_model, which = 2, main = "Normal Q-Q")

# Histogram of Residuals
hist(residuals(mlr_model), breaks = 30, main = "Histogram of Residuals", xlab = "Residuals")

```
There is a slight curve in the Residuals vs Fitted Plot suggeesting that the model is missing some non-linear relationships.

Deviations at the Ends in the QQ plot suggests suggest skewness or heavy tails in the residuals.

## Instrumental Variables (IV) 

### First Stage
```{r}
# Ensure renovated is numeric
dataset_train$renovated <- as.numeric(as.character(dataset_train$renovated))

# First Stage Regression
first_stage_model <- lm(renovated ~ grade + quartile_zone, data = dataset_train)

# Display summary of the first stage regression
summary(first_stage_model)
```
#### Interpretation
- The **Intercept** is significant, indicating that when all predictors are zero, the baseline value of `renovated` is approximately 0.0473.
- The variable `grade` has a negative and significant effect on `renovated`, with a small but statistically significant estimate (-0.0082, p < 0.001).
- The instrument `quartile_zone` shows mixed results:
  - **quartile_zone3** and **quartile_zone4** have significant positive coefficients (p < 0.001), indicating that properties in these zones are more likely to be renovated.
  - **quartile_zone2**, however, is not significant (p = 0.288).

- **Residual Standard Error (RSE)**: 0.193  
  The residuals indicate that the model captures only a small amount of variation in `renovated`.
  
- **Multiple R-squared**: 0.003466  
  This value indicates that only ~0.35% of the variation in `renovated` is explained by `grade` and `quartile_zone`. While low, this is not uncommon in first-stage regressions for binary variables.

- **Adjusted R-squared**: 0.003172  
  Adjusted for the number of predictors, the R-squared value remains very low.

- **F-statistic**: 11.82 (p < 0.001)  
  The F-statistic confirms that the model is statistically significant overall, suggesting that the predictors, collectively, have some explanatory power.

The first-stage regression results demonstrate partial **relevance** for the instrument `quartile_zone`. While `quartile_zone3` and `quartile_zone4` are significant predictors of `renovated`, `quartile_zone2` is not. The F-statistic exceeds the threshold of 10, supporting the instrument's overall strength in explaining `renovated`.
### Second Stage
```{r}
# Add predicted values of renovated from the first stage
dataset_train$hat_R <- predict(first_stage_model)

# Second Stage Regression
second_stage_model <- lm(price ~ bedrooms + grade + hat_R + nice_view +
                           living_in_m2 + perfect_condition + quartile_zone, 
                         data = dataset_train)

# Display summary of the second stage regression
summary(second_stage_model)
```
####  Interpretation
- **Residual Standard Error (RSE)**: 0.01687  
  This indicates a close fit of the model to the data, with small residuals overall.
  
- **Multiple R-squared**: 0.7565  
  Approximately 75.65% of the variation in house prices is explained by the predictors in the model.
  
- **Adjusted R-squared**: 0.7564  
  This value accounts for the number of predictors in the model, confirming a strong fit even when adjusted for complexity.
  
- **F-statistic**: 5279 (p < 2.2e-16)  
  The F-statistic is highly significant, suggesting that the model explains a substantial portion of the variation in house prices.

The second-stage regression demonstrates that the instrumented variable `hat_R` (renovation status) has a strong positive effect on house prices, supporting the hypothesis that renovations increase property value. All other variables are statistically significant, and the model explains a large portion of the variance in house prices. The results suggest that the instrumented renovations are indeed a key determinant of house prices, and the IV method provides a reliable estimate of their causal effect.
```{r}
plot(second_stage_model, which = 1, main = "Residuals vs Fitted")
plot(second_stage_model, which = 2, main = "Normal Q-Q")
hist(residuals(second_stage_model), breaks = 30, main = "Histogram of Residuals", xlab = "Residuals")

```
Both the residuals vs. fitted plot and the Q-Q plot suggest that the model might benefit from further refinement. The curvature in the residuals suggests possible non-linear relationships, while the non-normality of the residuals indicates the need for additional steps to improve the model fit and the validity of the estimates.

## Fixed-Effects Model

```{r}
# Fit the Fixed-effects Model
fixed_effects_model <- lm(price ~ bedrooms + living_in_m2 + nice_view + perfect_condition + quartile_zone, data = dataset_train)

# Display summary of the fixed-effects model
summary(fixed_effects_model)
```
### Interpretation
The fixed-effects model was estimated to account for unobserved location-specific factors that may influence house prices. The results are as follows:

- **Intercept**: The baseline house price is estimated at \$2.487, which is highly significant.
- **Bedrooms**: The number of bedrooms has a negligible and statistically insignificant effect on house prices (p-value = 0.988).
- **Living Area**: Each additional square meter of living area increases the house price by 0.0245%, and this effect is highly significant (p-value < 2e-16).
- **Nice View**: Having a nice view increases house prices by 1.57%, which is highly significant (p-value < 2e-16).
- **Perfect Condition**: Properties in perfect condition increase in price by approximately 0.68%, with a highly significant p-value (p-value < 2e-16).
- **Quartile Zones**: The coefficients for quartile zones indicate significant location-based effects:
  - Quartile Zone 2: +2.39% higher price.
  - Quartile Zone 3: +4.71% higher price.
  - Quartile Zone 4: +5.49% higher price.
  
Overall, the fixed-effects model provides strong evidence that property features such as living area, view, and condition, along with the neighborhood zone, are significant determinants of house prices.

The model has a residual standard error of 0.01751 and explains approximately 73.75% of the variance in house prices (R-squared = 0.7375). The F-statistic is 5458, confirming that the model is statistically significant.


## Sensivity Analysis for each Model

```{r}
# 1. Add interaction term (e.g., living area and number of bedrooms)
mlr_interaction_model <- lm(price ~ bedrooms * living_in_m2 + nice_view + perfect_condition + quartile_zone, data = dataset_train)
summary(mlr_interaction_model)

# 2. Log transformation of the dependent variable (price)
mlr_log_model <- lm(log(price) ~ bedrooms + living_in_m2 + nice_view + perfect_condition + quartile_zone, data = dataset_train)
summary(mlr_log_model)

# 3. Exclude control variable (e.g., remove `nice_view` variable)
mlr_reduced_model <- lm(price ~ bedrooms + living_in_m2 + perfect_condition + quartile_zone, data = dataset_train)
summary(mlr_reduced_model)
```




